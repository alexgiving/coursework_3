{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import lib for saving model and encoder\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Import preprocessing functions\n",
    "from sklearn.tree               import plot_tree\n",
    "from sklearn.preprocessing      import LabelEncoder\n",
    "from sklearn.model_selection    import train_test_split\n",
    "\n",
    "# Import confusion_matrix and roc_auc_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# Import metrics functions\n",
    "from sklearn.metrics import f1_score,                           \\\n",
    "                            r2_score,                           \\\n",
    "                            recall_score,                       \\\n",
    "                            accuracy_score,                     \\\n",
    "                            precision_score,                    \\\n",
    "                            mean_squared_error,                 \\\n",
    "                            mean_absolute_error,                \\\n",
    "                            balanced_accuracy_score,            \\\n",
    "                            precision_recall_fscore_support\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_path = \"build_files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "data = pd.read_excel('src/dataset_norm.xlsx')\n",
    "\n",
    "# Define minimal mark\n",
    "min_mark = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output distribution of scores\n",
    "x = range(len(data['Средний балл']))\n",
    "y = data['Средний балл']\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: lines in graph illustrate custom increase the number of not pass students.\n",
    "That trick were done for better model training. It helps to increase accuracy metricks of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out how many students did not pass the exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_print():\n",
    "    passed = 0\n",
    "    not_passed = 0\n",
    "    res = 0\n",
    "    for mark in data['Средний балл']:\n",
    "        if mark >= min_mark:\n",
    "            passed += 1\n",
    "        elif mark < min_mark:\n",
    "            not_passed += 1\n",
    "        res += 1\n",
    "    print(f\"TOTAL: {res}\\nPassed: {passed}\\nNot pas: {not_passed}\")\n",
    "\n",
    "\n",
    "counter_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replase mark of student by belonging to the class of successfully passed\n",
    "data['Сдал'] = pd.cut(x=data['Средний балл'], bins=[0, min_mark, 10], labels=[0, 1])\n",
    "data.drop(['Средний балл'], axis = 1, inplace= True)\n",
    "\n",
    "# Drop indicators which are not connected with extracurricular activities\n",
    "data.drop([\n",
    "            'Дата прохождения теста',\n",
    "            '15.Образование Ваших родителей?', \n",
    "            '3.С какими оценками Вы закончили школу?',\n",
    "            '4.Ходили ли Вы на подготовительные курсы перед поступлением в вуз?',\n",
    "            '7.Какая у Вас семья?',\n",
    "            '10.Получали ли Вы стипендию? (в течение последнего года)',\n",
    "            '11.Оцените, как Вам нравится учиться?',\n",
    "            '13.На какие средства Вы живете?',\n",
    "            '5.Брали ли Вы академический отпуск?',\n",
    "            '17.Укажите Ваше семейное положение.',\n",
    "            '12.Каковы условия Вашего проживания?'\n",
    "            ],\n",
    "            axis = 1, inplace = True)\n",
    "\n",
    "# That part index all names for privacy reasons\n",
    "name_hash = []\n",
    "for idx, name in enumerate(data['Учащийся']):\n",
    "    name_hash.append((idx, name))\n",
    "    data.loc[idx,'Учащийся'] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset with concern questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our data in 3 parts for train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(test_ratio, validation_ratio):\n",
    "# Replace all text output to index\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_data = data.copy()\n",
    "    s = (label_data.dtypes == 'object')\n",
    "    object_cols = list(s[s].index)\n",
    "    create(build_path)\n",
    "    for col in object_cols:\n",
    "        label_encoder.fit(label_data[col])\n",
    "        label_data[col] = label_encoder.transform(label_data[col])\n",
    "        # Save lable encouder hash to separate files to use them in future if need\n",
    "        file_name = f'{build_path}/{col.replace(\"/\", \"-\")}_class_linear_encoder.npy'\n",
    "        f = open(file_name, 'w+')\n",
    "        np.save(file_name, label_encoder.classes_)\n",
    "        f.close()\n",
    "\n",
    "# Split dataset\n",
    "    # NOTE: To fix split selections use random_state= parameter\n",
    "    size = label_data.shape[0]\n",
    "    validation_ratio = (validation_ratio * size) / (size * (1 - test_ratio))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(label_data.drop('Сдал', axis = 1), \n",
    "                                                        label_data['Сдал'], \n",
    "                                                        test_size=test_ratio)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                      y_train, \n",
    "                                                      test_size=validation_ratio)\n",
    "\n",
    "    # Output X_train.iloc[:,1:] delete student hash from training and testing selections\n",
    "    return X_train.iloc[:,1:], X_test.iloc[:,1:], X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(test_ratio=0.25, validation_ratio=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model variants\n",
    "In that part I will choose the classification model which will predict if student pass exams successfully or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_quality(y_test, y_pred):\n",
    "    print(\"Accuracy:\",          accuracy_score(y_test, y_pred))\n",
    "    print(\"Recall:\",            recall_score(y_test, y_pred, average='macro'))\n",
    "    print(\"Precision:\",         precision_score(y_test, y_pred, average='macro'))\n",
    "    print(\"F1:\",                f1_score(y_test, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "    print(\"Weighted Recall:\",   (precision_recall_fscore_support(y_test, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For historical reason\n",
    "def regression_quality(y_test, y_pred):\n",
    "    print(\"MSE:\",               mean_squared_error(y_test, y_pred))\n",
    "    print(\"RMSE:\",              mean_squared_error(y_test, y_pred)**(1/2))\n",
    "    print(\"MAE:\",               mean_absolute_error(y_test, y_pred))\n",
    "    print(\"R2:\",                r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = ['Accuracy', 'Balanced Accuracy', 'Recall', 'Precision', 'F1', 'MSE', 'RMSE', 'MAE', 'R2']\n",
    "model_array  = []\n",
    "output_array = []\n",
    "\n",
    "def compilance_print(model, y_test, y_pred, model_flag):\n",
    "    temp_array = []\n",
    "    if model_flag == 'cls': # Classifier\n",
    "        temp_array.append(accuracy_score(y_test, y_pred))\n",
    "        temp_array.append(balanced_accuracy_score(y_test, y_pred))\n",
    "        temp_array.append(recall_score(y_test, y_pred, average='macro'))\n",
    "        temp_array.append(precision_score(y_test, y_pred, average='macro'))\n",
    "        temp_array.append(f1_score(y_test, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "        for _ in range(4): temp_array.append(None) # Set regression metrics as None\n",
    "    \n",
    "    # For historical reason\n",
    "    elif model_flag == 'reg': # Regression\n",
    "        for _ in range(5): temp_array.append(None) # Set classifier metrics as None\n",
    "        temp_array.append(mean_squared_error(y_test, y_pred))\n",
    "        temp_array.append(mean_squared_error(y_test, y_pred)**(1/2))\n",
    "        temp_array.append(mean_absolute_error(y_test, y_pred))\n",
    "        temp_array.append(r2_score(y_test, y_pred))\n",
    "    else: \n",
    "        print('Error')\n",
    "        for _ in metrics_list: temp_array.append(None)\n",
    "\n",
    "    flag = 1\n",
    "    model_indx = -1\n",
    "    for indx, _model in enumerate(model_array):\n",
    "        if _model == model:\n",
    "            model_indx = indx\n",
    "            flag = 0\n",
    "    if flag:\n",
    "        output_array.append([0]* len(metrics_list))\n",
    "        model_indx = len(model_array)\n",
    "        model_array.append(model)\n",
    "    for indx, el in enumerate(temp_array):\n",
    "        output_array[model_indx][indx] = el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_show(model, X, y):\n",
    "    metrics.plot_confusion_matrix(model, X, y)\n",
    "    metrics.plot_roc_curve(model, X, y)  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perception Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "Perc = Perceptron()\n",
    "Perc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Perc.predict(X_test)\n",
    "\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('Perceptron', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RanF = RandomForestClassifier()\n",
    "RanF.fit(X_train, y_train)\n",
    "\n",
    "y_pred = RanF.predict(X_test)\n",
    "\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('RandomForest', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GraB = GradientBoostingClassifier()\n",
    "GraB.fit(X_train, y_train)\n",
    "\n",
    "y_pred = GraB.predict(X_test)\n",
    "\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('GradientBoosting', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MulNB = MultinomialNB()\n",
    "MulNB.fit(X_train, y_train)\n",
    "\n",
    "y_pred = MulNB.predict(X_test)\n",
    "\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('MultinomialNB', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComplementNB Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "ComNB = ComplementNB()\n",
    "ComNB.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ComNB.predict(X_test)\n",
    "\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('ComplementNB', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "GaNB = GaussianNB()\n",
    "GaNB.fit(X_train, y_train)\n",
    "\n",
    "y_pred = GaNB.predict(X_test)\n",
    "\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('GaussianNB', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BernoulliNB Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "BerNB = BernoulliNB()\n",
    "BerNB.fit(X_train, y_train)\n",
    "\n",
    "y_pred = BerNB.predict(X_test)\n",
    "\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('BernoulliNB', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('SVC', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC()\n",
    "lsvc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lsvc.predict(X_test)\n",
    "\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('LinearSVC', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtr = DecisionTreeClassifier()\n",
    "dtr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dtr.predict(X_test)\n",
    "\n",
    "plot_tree(dtr)\n",
    "\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('DecisionTreeClassifier', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreeClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "extr = ExtraTreeClassifier()\n",
    "extr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = extr.predict(X_test)\n",
    "\n",
    "plot_tree(extr)\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('ExtraTreeClassifier', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLPCl = MLPClassifier()\n",
    "MLPCl.fit(X_train, y_train)\n",
    "\n",
    "y_pred = MLPCl.predict(X_test)\n",
    "\n",
    "classification_quality(y_test, y_pred)\n",
    "compilance_print('MLPClassifier', y_test, y_pred, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix\n",
    "That table helps to understand which model is better for my work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(index=model_array, columns=metrics_list, data=output_array).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = dtr  # My own prefix of DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use DecisionTree Classifier model due to its metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = f'{build_path}/DecisionTreeClassifier.sav'\n",
    "pickle.dump(my_model, open(filename, 'wb'))\n",
    "\n",
    "#my_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output confusion matrix and ROC/AUC curve\n",
    "graph_show(my_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That metrics shows the accuracy of model. As we can see that models return very few False-Positive and True-Negative errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted results of validation selection\n",
    "# NOTE: Validation selection was not used for training model thus the results are objective\n",
    "# Also X_val.iloc[:,1:] used for hiding from model hash of students names\n",
    "y_pred_val = my_model.predict(X_val.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions allow to output results of validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_with_name():\n",
    "    print(f'Студент {name_hash[X_val.iloc[i, 0]][1]} предположительно {\"сдал(а)\" if y_pred_val[i] == 1 else \"не сдал(а)\"}, в жизни {\"сдал(а)\" if y_val.iloc[i] == 1 else \"не сдал(а)\"}')\n",
    "\n",
    "def print_with_id():\n",
    "    print(f'Студент №{i+1} предположительно {\"сдал(а)\" if y_pred_val[i] == 1 else \"не сдал(а)\"}, в жизни {\"сдал(а)\" if y_val.iloc[i] == 1 else \"не сдал(а)\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_counter = 0\n",
    "total = 0\n",
    "for i in range(len(y_pred_val)):\n",
    "    if y_pred_val[i] != y_val.iloc[i] == 1:\n",
    "        error_counter += 1\n",
    "    total += 1\n",
    "    print_with_id()\n",
    "print(f'There were {error_counter} from {total} error(s)!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation with my answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_questions = [\"Учащийся\", \"16.Работаете ли Вы?\", \"14.Увлекаетесь ли Вы спортом?\", \n",
    "                \"9.Сколько времени Вы уделяете самостоятельной подготовке к занятиям (в среднем)?\",\n",
    "                \"8.Как много Вы пропускаете аудиторных занятий?\", \"6.Бывают ли у Вас долги по экзаменам/зачетам?\", \n",
    "                \"2.Посещаете ли Вы дополнительные занятия (неважно, в вышке или вне)?\", \"1.Участвуете ли Вы в олимпиадах?\" ]\n",
    "\n",
    "\n",
    "my_answers =    [\n",
    "                \"Алексей\",                                  # Учащийся\n",
    "                \"Да\",                                       # 16.Работаете ли Вы?\n",
    "                \"да, хожу на фитнес или в тренажерный зал\", # 14.Увлекаетесь ли Вы спортом?\n",
    "                \"Готовлюсь только перед занятиями\",         # 9.Сколько времени Вы уделяете самостоятельной подготовке к занятиям (в среднем)?\n",
    "                \"Не пропускаете\",                           # 8.Как много Вы пропускаете аудиторных занятий?\n",
    "                \"Нет\",                                      # 6.Бывают ли у Вас долги по экзаменам/зачетам?\n",
    "                \"нет\",                                      # 2.Посещаете ли Вы дополнительные занятия (неважно, в вышке или вне)?\n",
    "                \"Нет\",                                      # 1.Участвуете ли Вы в олимпиадах?\n",
    "                ]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=[my_answers], columns=my_questions)\n",
    "\n",
    "for col in pd.DataFrame(df.iloc[:,1:]).columns:             # Iterate by df without name\n",
    "    encoder = LabelEncoder()\n",
    "    filename = f\"{build_path}/{str(col).replace('/', '-')}_class_linear_encoder.npy\" \n",
    "    encoder.classes_ = np.load(filename, allow_pickle=True) # Import encouder fit data\n",
    "    df[col] = encoder.transform(df[col])\n",
    "\n",
    "\n",
    "y_pred_my = my_model.predict(df.iloc[:,1:])                 # Without name\n",
    "print(f'{df.iloc[0,0]} предположительно {\"сдал(а)\" if y_pred_my == 1 else \"не сдал экзамен\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(path):\n",
    "    try:\n",
    "        os.stat(path)\n",
    "    except OSError:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all text output to index\n",
    "# Used for outputing correlation matrix\n",
    "label_encoder = LabelEncoder()\n",
    "label_data = data.iloc[:,1:].copy()\n",
    "\n",
    "for col in label_data.columns:\n",
    "    filename = f\"{build_path}/{col.replace('/', '-')}_class_linear_encoder.npy\"\n",
    "    if exists(filename):\n",
    "        label_encoder.classes_ = np.load(filename, allow_pickle=True)\n",
    "    else: \n",
    "        label_encoder.fit(data[col])\n",
    "#   always:\n",
    "    label_data[col] = label_encoder.transform(label_data[col])\n",
    "\n",
    "\n",
    "# Creating correlation matrix\n",
    "rs = np.random.RandomState(0)\n",
    "corr = label_data.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm', axis=None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbc59c1602769476583de30dcfaf94a717f95996dfb09063277734c10faa726c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
